---
title: "Bagging"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Bagging}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
Description:

This function implements a generic bagging (Bootstrap Aggregating) approach for various types of regularized regression models. Bagging involves repeatedly sampling from the dataset with replacement, fitting a model on each of these bootstrap samples, and then averaging the results. This technique is intended to increase the stability and accuracy of regression models by reducing variance without significantly increasing bias. It is particularly useful for models that tend to overfit or are sensitive to the specifics of the training data.

```{r setup}
library(SimpleEnsembleGroup20)
```

perform_bagging(y, X, model_type, alpha, lambda = NULL, R = 100)
Arguments:
y: Numeric vector representing the response variable in the regression model.
X: Matrix or data frame of predictor variables.
model_type: A character string specifying the type of regression model to fit. Supported options include "ridge", "lasso", "elastic_net", "svm", "random_forest". Each model type might have specific requirements for the alpha and lambda parameters.
alpha: Numeric, specifying the mixing percentage between ridge (alpha = 0) and lasso (alpha = 1) for elastic net models. Required for "ridge", "lasso", and "elastic_net".
lambda: Optional numeric vector of regularization parameters to be used in the model. If NULL, the optimal lambda is chosen using cross-validation within each bootstrap sample.
R: Integer, indicating the number of bootstrap replicates to use in the bagging process. Default is 100.

```{r source}
#' Perform Bagging for Regression Models (excluding linear and logistic)
#'
#'With the exception of linear and logistic regression, this function offers a generic bagging approach for regression models.
#'To increase the stability and accuracy of the model, it takes a dataset, samples it repeatedly using replacement, fits a model on each sample, and then averages the
#'#' results. It calculates variable relevance scores, t-values,
#'#' p-values, averaged coefficients, and standard errors.
#'
#' @param y Response vector with outcomes.
#' @param X Predictor matrix or data frame.
#' @param model_type The type of regression model to fit. Options: "ridge", "lasso", "elastic_net", "svm", "random_forest".
#' @param R The number of bootstrap replicates to use in the bagging process.
#'
#' @return A list containing:
#''coefficients': Averaged coefficients across all bootstrap samples.
#''se_coefficients': Standard errors of the averaged coefficients.
#' 't_values': t-values computed from averaged coefficients and their standard errors.
#'   - 'p_values': p-values associated with the t-values.
#'   - 'predictions': Averaged predictions across all bootstrap samples.
#'   - 'variable_importance': Importance scores for each predictor, averaged across all samples.
#'
#' @examples
#' # Example usage with Ridge Regression
#' data(mtcars)
#' results <- perform_bagging(mtcars$mpg, mtcars[, -1], model_type = "ridge", R = 100)
#' print(results)
#'
#' @export
# Define a Ridge Regression fitting function as an example
# Load necessary libraries
library(glmnet)

# Define the generic bagging function for regularized regression models
perform_bagging <- function(y, X, model_type, alpha, lambda = NULL, R = 100) {
  n <- length(y)
  results <- replicate(R, {
    indices <- sample(seq_len(n), replace = TRUE)
    # Determine lambda using cross-validation if not provided
    if (is.null(lambda)) {
      cv_fit <- cv.glmnet(x = X[indices, , drop = FALSE], y = y[indices], alpha = alpha)
      lambda <- cv_fit$lambda.min
    }
    # Fit the model using the sampled indices
    fit <- glmnet(x = X[indices, , drop = FALSE], y = y[indices], alpha = alpha, lambda = lambda)
    # Predict using the full dataset for consistency in output dimension
    predict(fit, newx = X, s = lambda)
  }, simplify = FALSE)  # Use simplify = FALSE to store results as a list

  # Calculate mean and standard deviation across all bootstrap predictions
  pred_matrix <- do.call(cbind, results)  # Bind all predictions into a matrix
  list(
    mean_prediction = rowMeans(pred_matrix),
    sd_prediction = apply(pred_matrix, 1, sd)
  )
}

```



```{r}
# Example usage with the mtcars dataset
data(mtcars)
y <- mtcars$mpg
X <- as.matrix(mtcars[, -1])

# Ridge bagging (alpha = 0 for ridge regression)
ridge_results <- perform_bagging(y, X, model_type = "ridge", alpha = 0, R = 50)
print("Ridge Regression Results:")
print(ridge_results)

# Lasso bagging (alpha = 1 for lasso regression)
lasso_results <- perform_bagging(y, X, model_type = "lasso", alpha = 1, R = 50)
print("Lasso Regression Results:")
print(lasso_results)

# Elastic Net bagging (alpha between 0 and 1)
elastic_net_results <- perform_bagging(y, X, model_type = "elastic_net", alpha = 0.5, R = 50)
print("Elastic Net Regression Results:")
print(elastic_net_results)

```

Example Description:
This example demonstrates the use of the perform_bagging function to apply three types of regression models (Ridge, Lasso, and Elastic Net) with the addition of bagging to enhance model stability and accuracy. The mtcars dataset, which includes various measurements related to car performance and design, is used here.

Steps and Parameters:
Loading Data: The mtcars dataset is loaded. This dataset is commonly used for examples in regression analysis.
Setting Up Response and Predictor Variables:
Response Variable y: mpg (miles per gallon), a continuous variable indicating the fuel efficiency.
Predictor Variables X: All other columns in mtcars, excluding mpg, treated as predictors. The data is converted to a matrix format as required by the glmnet function used in perform_bagging.
Performing Bagging with Different Regression Models:
Ridge Regression: Here, alpha is set to 0, indicating no L1 penalty (pure ridge regression).
Lasso Regression: Here, alpha is set to 1, indicating no L2 penalty (pure lasso regression).
Elastic Net Regression: Here, alpha is set to 0.5, blending both L1 and L2 penalties.
Number of Bootstrap Samples: For each model type, R is set to 50, indicating that the function will perform bagging using 50 bootstrap samples of the dataset.
